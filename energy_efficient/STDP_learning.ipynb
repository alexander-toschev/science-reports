{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f620028-87ed-4c99-a115-950ce07e04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class LatencyEncoder:\n",
    "    def __init__(self, time: int = 100):\n",
    "        self.time = time  # Общее число временных шагов\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        image: Tensor [1, 28, 28] или [28, 28], значения от 0 до 1 или до 255\n",
    "        return: spike_tensor [time, 1, 784]\n",
    "        \"\"\"\n",
    "        if image.ndim == 3:\n",
    "            image = image.squeeze()\n",
    "\n",
    "        if image.max() > 1:\n",
    "            image = image / 255.0\n",
    "\n",
    "        spike_tensor = torch.zeros((self.time, 1, 784))\n",
    "\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                pixel = image[i, j].item()\n",
    "                if pixel > 0:\n",
    "                    spike_time = int((1.0 - pixel) * (self.time - 1))\n",
    "                    spike_tensor[spike_time, 0, i * 28 + j] = 1.0\n",
    "\n",
    "        return spike_tensor.view(self.time, 1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1080659-6ab0-4b4c-9dc8-1a070c04babe",
   "metadata": {},
   "source": [
    "# 🧠 Spiking Neural Network (SNN) на базе BindsNET с обучением через STDP\n",
    "\n",
    "В этом эксперименте реализована простая биологически правдоподобная спайковая нейросеть (SNN) для обработки изображений MNIST.\n",
    "\n",
    "## 📌 Архитектура сети:\n",
    "- **Входной слой (`Input`, 784 нейрона)** — по одному нейрону на каждый пиксель изображения 28×28.\n",
    "- **Poisson-кодировщик** — преобразует яркость пикселей в вероятностные временные спайки.\n",
    "- **Полносвязный слой (`Connection`)** — соединяет вход с выходом (матрица весов 784 × 100).\n",
    "- **Выходной слой (`LIF`, 100 нейронов)** — Leaky Integrate-and-Fire нейроны с утечкой и порогом.\n",
    "- **STDP (Spike-Timing Dependent Plasticity)** — обучение без градиентов; веса усиливаются, если вход активен до выходного спайка.\n",
    "\n",
    "## 🔬 Что делается:\n",
    "1. Загружается одно изображение MNIST.\n",
    "2. Кодируется в Poisson-спайковый поток.\n",
    "3. Пропускается через сеть:\n",
    "   - `Input` получает входные спайки,\n",
    "   - `LIF` нейроны активируются в зависимости от весов.\n",
    "4. Сохраняются:\n",
    "   - Спайковая активность `LIF`-нейронов до и после подачи входа.\n",
    "   - Сумма спайков на входе (`Input`) — показывает, какие пиксели активны.\n",
    "   - Веса одного выбранного `LIF`-нейрона — до и после STDP.\n",
    "\n",
    "## 📈 Визуализация:\n",
    "- График: сравнение спайковой активности нейронов до/после + входные спайки.\n",
    "- График: изменение весов, ведущих к нейрону `LIF[42]` — видно, как STDP усиливает значимые связи.\n",
    "\n",
    "## 🎯 Цель:\n",
    "Показать, как SNN:\n",
    "- преобразует изображение в поток спайков,\n",
    "- активирует только специфичные нейроны,\n",
    "- адаптирует веса на основе временных шаблонов (STDP), без использования обратного распространения ошибки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62625648-17a4-47af-9950-dfa00b772a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== SETUP (Colab/Local) ======\n",
    "# !pip -q install bindsnet==0.2.8 torchvision==0.18.1 torch==2.3.1 --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "import os, itertools, random, csv, time as _ptime\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== Utils ======\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "def to_2d(s):  # [T,N] or [T,1,N] -> [T,N]\n",
    "    return s[:,0,:] if (s.dim()==3 and s.size(1)==1) else s\n",
    "\n",
    "# ====== Config ======\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # core\n",
    "    time:   int = 200\n",
    "    n_hidden: int = 100\n",
    "    nu_plus:  float = 0.02\n",
    "    nu_minus: float = -0.02\n",
    "\n",
    "    # inhibition / WTA\n",
    "    inhib_strength: float = 0.3\n",
    "    inh_decay: float = 0.9\n",
    "    top_k: int = 0                    # 0 = WTA off for diagnostics\n",
    "    enable_inhibition_at_start: bool = False\n",
    "\n",
    "    # encoder\n",
    "    encoder: str = \"latency\"         # start with Poisson to \"ignite\" spikes\n",
    "\n",
    "    # homeostasis\n",
    "    target_spikes: float = 2.0\n",
    "    eta_up: float = 1.0\n",
    "    eta_down: float = 0.5\n",
    "    thresh_min: float = 0.2\n",
    "    thresh_max: float = 2.0\n",
    "    thresh_init: float = 0.5          # v_thresh initial (BindsNET positive scale)\n",
    "\n",
    "    # weights\n",
    "    w_clip_min: float = 0.0\n",
    "    w_clip_max: float = 1.5\n",
    "    w_col_target_norm: float = 20.0\n",
    "    w_init_lo: float = 0.8\n",
    "    w_init_hi: float = 1.2\n",
    "\n",
    "    # loop\n",
    "    N: int = 200\n",
    "    log_every: int = 50\n",
    "    seed: int = 42\n",
    "\n",
    "# ====== Helpers: WTA, norm, thresholds, plots, metrics ======\n",
    "def apply_wta(s, top_k=1):\n",
    "    s2 = to_2d(s)\n",
    "    sb = s2.sum(0).float().squeeze()\n",
    "    if sb.sum() == 0:\n",
    "        return False, None\n",
    "    vals, idxs = torch.topk(sb, k=min(top_k, sb.numel()))\n",
    "    s.zero_()\n",
    "    for j in idxs.tolist():\n",
    "        if s.dim()==3:\n",
    "            s[:,0,j] = True\n",
    "        else:\n",
    "            s[:,j] = True\n",
    "    return True, idxs.tolist()\n",
    "\n",
    "def weight_soft_bound_and_colnorm(conn_w, w_clip_min, w_clip_max, target_norm):\n",
    "    with torch.no_grad():\n",
    "        w = conn_w.data\n",
    "        w.clamp_(w_clip_min, w_clip_max)\n",
    "        col_norm = w.norm(p=1, dim=0, keepdim=True) + 1e-6\n",
    "        w.mul_(target_norm / col_norm)\n",
    "\n",
    "def adapt_thresholds(layer, spike_counts, cfg: Cfg):\n",
    "    with torch.no_grad():\n",
    "        vt = layer.v_thresh if hasattr(layer, \"v_thresh\") else layer.thresh\n",
    "        vt -= 0.05 * (spike_counts < 1.0).float()        # if silent -> lower threshold\n",
    "        vt += 0.02 * (spike_counts > 3.0).float()        # if too active -> raise\n",
    "        vt.clamp_(cfg.thresh_min, cfg.thresh_max)\n",
    "        if hasattr(layer, \"v_thresh\"): layer.v_thresh = vt\n",
    "        else: layer.thresh = vt\n",
    "\n",
    "def spiking_metrics_window(lif_s, winners=None):\n",
    "    s = to_2d(lif_s).to(torch.bool)\n",
    "    T, N = s.shape\n",
    "    per_n = s.sum(0)\n",
    "    tot = int(per_n.sum())\n",
    "    active = int((per_n > 0).sum())\n",
    "    if tot > 0:\n",
    "        p = (per_n / tot).float().cpu().numpy()\n",
    "        HHI = float((p**2).sum())\n",
    "        ps = np.sort(p)\n",
    "        Gini = float((np.cumsum(ps).sum()/ps.sum() - (len(ps)+1)/2)/len(ps))\n",
    "    else:\n",
    "        HHI, Gini = 1.0, 1.0\n",
    "    uniq_winners = len(set(winners)) if winners else 0\n",
    "    return dict(T=T, N=N, total_spikes=tot, active=active, HHI=HHI, Gini=Gini, uniq_winners=uniq_winners)\n",
    "\n",
    "class SNNMeter:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self):\n",
    "        self.samples=0; self.S_out=0; self.S_in=0; self.SynOps=0; self.V_updates=0\n",
    "        self.usage_counts = {}\n",
    "    def log_sample(self, lif_s, in_s, n_hidden, T, winners=None):\n",
    "        lif2 = to_2d(lif_s);  in2 = to_2d(in_s)\n",
    "        s_out = int(lif2.sum().item())\n",
    "        s_in  = int(in2.sum().item())\n",
    "        self.S_out += s_out; self.S_in += s_in\n",
    "        self.SynOps += s_in * n_hidden\n",
    "        self.V_updates += n_hidden * T\n",
    "        self.samples += 1\n",
    "        if winners:\n",
    "            for j in winners:\n",
    "                self.usage_counts[j] = self.usage_counts.get(j,0)+1\n",
    "    def report(self, a=1.0, b=0.05, c=0.005):\n",
    "        s = max(1, self.samples)\n",
    "        HHI_win = 0.0\n",
    "        if self.usage_counts:\n",
    "            tot = sum(self.usage_counts.values())\n",
    "            ps = np.array([v/tot for v in self.usage_counts.values()], dtype=float)\n",
    "            HHI_win = float((ps**2).sum())\n",
    "        return {\n",
    "            \"spikes_per_sample\": self.S_out/s,\n",
    "            \"synops_per_sample\": self.SynOps/s,\n",
    "            \"v_updates_per_sample\": self.V_updates/s,\n",
    "            \"energy_proxy_per_sample\": (a*self.S_out + b*self.SynOps + c*self.V_updates)/s,\n",
    "            \"winners_unique\": len(self.usage_counts),\n",
    "            \"winner_HHI\": HHI_win,\n",
    "        }\n",
    "\n",
    "# ====== Build Net & Encoder ======\n",
    "from bindsnet.network import Network\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.learning import PostPre\n",
    "from torchvision import transforms\n",
    "from bindsnet.datasets import MNIST\n",
    "from bindsnet.network.monitors import Monitor\n",
    "\n",
    "def build_net(cfg: Cfg):\n",
    "    net = Network()\n",
    "\n",
    "    input_layer = Input(n=784, shape=(1, 28, 28), traces=True)\n",
    "    lif_layer   = LIFNodes(n=cfg.n_hidden, traces=True)\n",
    "\n",
    "    net.add_layer(input_layer, name='Input')\n",
    "    net.add_layer(lif_layer,   name='LIF')\n",
    "\n",
    "    connection = Connection(source=input_layer, target=lif_layer)\n",
    "    connection.update_rule = PostPre(connection=connection, nu=(cfg.nu_plus, cfg.nu_minus))\n",
    "    net.add_connection(connection, source='Input', target='LIF')\n",
    "\n",
    "    # Lateral inhibition (created, but optionally disabled at start)\n",
    "    W_inh = torch.full((cfg.n_hidden, cfg.n_hidden), -cfg.inhib_strength)\n",
    "    W_inh.fill_diagonal_(0.0)\n",
    "    recurrent_inh = Connection(source=lif_layer, target=lif_layer, w=W_inh.clone())\n",
    "    net.add_connection(recurrent_inh, source='LIF', target='LIF')\n",
    "\n",
    "    # Weights init (stronger to ignite)\n",
    "    with torch.no_grad():\n",
    "        connection.w.data.uniform_(cfg.w_init_lo, cfg.w_init_hi)\n",
    "\n",
    "    # Thresholds: use v_thresh if available\n",
    "    th0 = torch.full((cfg.n_hidden,), cfg.thresh_init)\n",
    "    if hasattr(lif_layer, \"v_thresh\"): lif_layer.v_thresh = th0.clone()\n",
    "    else: lif_layer.thresh = th0.clone()\n",
    "\n",
    "    # Optionally disable inhibition at start (for diagnostics)\n",
    "    if not cfg.enable_inhibition_at_start:\n",
    "        with torch.no_grad():\n",
    "            recurrent_inh.w.zero_()\n",
    "\n",
    "    return net, input_layer, lif_layer, connection, recurrent_inh, W_inh\n",
    "\n",
    "# Пре-процесс для произвольных изображений (PIL / np / tensor)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28), antialias=True),\n",
    "    transforms.ToTensor(),                 # -> [1,28,28] float in [0,1]\n",
    "    # Никаких Normalize(mean,std) здесь — нам нужны «сырые» 0..1!\n",
    "])\n",
    "\n",
    "def make_encoder(encoder_type: str, T: int, rate_floor: float = 1e-3):\n",
    "    \"\"\"\n",
    "    encoder_type: 'poisson' | 'latency'\n",
    "    T: окно по времени\n",
    "    rate_floor: минимальная интенсивность/страховка, чтобы не было полностью нулевого потока\n",
    "    \"\"\"\n",
    "    encoder_type = encoder_type.lower()\n",
    "    assert encoder_type in (\"poisson\", \"latency\")\n",
    "\n",
    "    def encode_poisson(img_tensor):\n",
    "        # img_tensor: [1,28,28] в [0,1]\n",
    "        rates = img_tensor.view(-1)                              # [784]\n",
    "        rates = torch.clamp(rates, 0.0, 1.0)\n",
    "        rates = torch.maximum(rates, torch.full_like(rates, rate_floor))\n",
    "        # Сэмплируем Бернулли на каждом такте: [T,784]\n",
    "        # Важно: не кумулятивный сумм, а независимые спайки\n",
    "        rand = torch.rand((T, rates.numel()))\n",
    "        spikes = (rand < rates).to(torch.float32)               # [T,784]\n",
    "        return spikes.view(T, 1, 784)                           # [T,1,784]\n",
    "\n",
    "    def encode_latency(img_tensor):\n",
    "        # Один импульс на пиксель в момент t ~ (1 - p) * (T-1)\n",
    "        x = img_tensor.squeeze(0)                               # [28,28]\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "        x = torch.maximum(x, torch.full_like(x, rate_floor))    # floor, чтобы пустые пиксели не ломали всё\n",
    "        spikes = torch.zeros((T, 1, 784), dtype=torch.float32)\n",
    "        # маппинг интенсивности в более поздние времена (ярче -> раньше)\n",
    "        t_idx = ((1.0 - x) * (T - 1)).round().to(torch.int64)   # [28,28]\n",
    "        # аккуратно раскладываем\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                t = int(t_idx[i, j])\n",
    "                if t >= 0 and t < T:\n",
    "                    spikes[t, 0, i * 28 + j] = 1.0\n",
    "        return spikes\n",
    "\n",
    "    if encoder_type == \"poisson\":\n",
    "        return encode_poisson, preprocess\n",
    "    else:\n",
    "        return encode_latency, preprocess\n",
    "        \n",
    "def _to_2d(s):  # [T, B, N] -> [T, N]\n",
    "    return s[:, 0, :] if s.dim()==3 else s\n",
    "# ====== One Experiment ======\n",
    "def run_experiment(cfg: Cfg, verbose=True):\n",
    "    set_seed(cfg.seed)\n",
    "    dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    net, input_layer, lif_layer, connection, recurrent_inh, W_inh = build_net(cfg)\n",
    "    lif_mon = Monitor(lif_layer, state_vars=(\"s\",), time=cfg.time)\n",
    "    inp_mon = Monitor(input_layer, state_vars=(\"s\",), time=cfg.time)\n",
    "    net.add_monitor(lif_mon, name=\"lif_mon\")\n",
    "    net.add_monitor(inp_mon, name=\"inp_mon\")\n",
    "    # выбираем тип явно\n",
    "    ENCODER_TYPE = cfg.encoder   # позже переключишь на 'latency'\n",
    "    T = cfg.time\n",
    "\n",
    "    encode, preprocess = make_encoder(ENCODER_TYPE, T)\n",
    "    meter = SNNMeter()\n",
    "    for i in range(cfg.N):\n",
    "        sample = dataset[i]\n",
    "        image  = sample[\"image\"]\n",
    "        spike_input = encoder(image)\n",
    "        si = spike_input\n",
    "        #print(\"encoder out shape:\", tuple(si.shape), \" sum:\", float(si.sum()))\n",
    "        inputs = {\"Input\": spike_input}\n",
    "\n",
    "        net.run(inputs=inputs, time=cfg.time)\n",
    "        # ==== ВАЖНО: берём полный растр за окно из мониторов ====\n",
    "        lif_s_full = lif_mon.get(\"s\")           # [T, B, N]\n",
    "        in_s_full  = inp_mon.get(\"s\")           # [T, B, 784]\n",
    "        lif2 = _to_2d(lif_s_full)               # [T, N]\n",
    "        in2  = _to_2d(in_s_full)                # [T, 784]\n",
    "    \n",
    "        # Быстрая диагностика\n",
    "        if i in (0, 49, 99, 149, 199):\n",
    "            print(\"INPUT spikes sum (window):\", int(in2.sum().item()))\n",
    "            print(\"LIF   spikes sum (window):\", int(lif2.sum().item()))\n",
    "\n",
    "        winners = None\n",
    "        if cfg.top_k and cfg.top_k > 0:\n",
    "            # WTA on lif_layer.s (not on lif_s_full copy)\n",
    "            ok, winners = apply_wta(lif_layer.s, top_k=cfg.top_k)\n",
    "            if not ok:\n",
    "                net.reset_state_variables()\n",
    "                lif_mon.reset_state_variables()\n",
    "                inp_mon.reset_state_variables()\n",
    "                continue\n",
    "\n",
    "        # window metrics\n",
    "        m = spiking_metrics_window(lif_s_full, winners)\n",
    "        if verbose and ((i+1) % cfg.log_every == 0 or i==0):\n",
    "            print(f\"[{i+1}] total={m['total_spikes']} active={m['active']}/{m['N']} HHI={m['HHI']:.3f}\")\n",
    "\n",
    "        # homeostasis on raw spikes (pre-WTA)\n",
    "        spike_counts = to_2d(lif_s_full).sum(0).float().squeeze()\n",
    "        adapt_thresholds(lif_layer, spike_counts, cfg)\n",
    "\n",
    "        # weight norm\n",
    "        #weight_soft_bound_and_colnorm(connection.w, cfg.w_clip_min, cfg.w_clip_max, cfg.w_col_target_norm)\n",
    "\n",
    "        # energy logging\n",
    "        meter.log_sample(lif_s_full, in_s_full, cfg.n_hidden, cfg.time, winners=winners)\n",
    "\n",
    "        net.reset_state_variables()\n",
    "        lif_mon.reset_state_variables()\n",
    "        inp_mon.reset_state_variables()\n",
    "\n",
    "    rpt = meter.report()\n",
    "    out = {**asdict(cfg), **rpt}\n",
    "    return out\n",
    "\n",
    "# ====== Grid Runner (compact) ======\n",
    "def grid_run(base: Cfg):\n",
    "    grid = {\n",
    "        \"inhib_strength\": [0.3, 0.5],\n",
    "        \"top_k\": [0, 3],\n",
    "        \"time\": [200, 300],\n",
    "        \"use_latency\": [False, True],\n",
    "    }\n",
    "    keys, vals = zip(*grid.items())\n",
    "    results = []\n",
    "    t0 = _ptime.time()\n",
    "    for combo in itertools.product(*vals):\n",
    "        cfg = Cfg(**{**asdict(base), **dict(zip(keys, combo))})\n",
    "        print(\">>> run:\", {k: getattr(cfg,k) for k in keys})\n",
    "        res = run_experiment(cfg, verbose=False)\n",
    "        print({k: res[k] for k in [\"spikes_per_sample\",\"winners_unique\",\"winner_HHI\",\"energy_proxy_per_sample\"]})\n",
    "        results.append(res)\n",
    "\n",
    "    os.makedirs(\"out\", exist_ok=True)\n",
    "    csv_path = os.path.join(\"out\",\"snn_energy_accuracy_grid.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader(); writer.writerows(results)\n",
    "    print(f\"Saved: {csv_path} | runs={len(results)} | elapsed={_ptime.time()-t0:.1f}s\")\n",
    "\n",
    "    # quick Pareto-ish\n",
    "    best = (sorted(results, key=lambda r: (r[\"energy_proxy_per_sample\"], r[\"winner_HHI\"], -r[\"winners_unique\"])))[:5]\n",
    "    print(\"\\nTop-5 Pareto-ish:\")\n",
    "    for r in best:\n",
    "        print({k: r[k] for k in [\"inhib_strength\",\"top_k\",\"time\",\"use_latency\",\n",
    "                                 \"spikes_per_sample\",\"winners_unique\",\"winner_HHI\",\"energy_proxy_per_sample\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8eb2b6-28b4-499d-9679-d66f8b0c23c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT spikes sum (window): 166\n",
      "LIF   spikes sum (window): 0\n",
      "[1] total=0 active=0/100 HHI=1.000\n",
      "INPUT spikes sum (window): 184\n",
      "LIF   spikes sum (window): 100\n",
      "[50] total=100 active=100/100 HHI=0.010\n",
      "INPUT spikes sum (window): 84\n",
      "LIF   spikes sum (window): 0\n",
      "[100] total=0 active=0/100 HHI=1.000\n"
     ]
    }
   ],
   "source": [
    "cfg = Cfg(\n",
    "    time=200,\n",
    "    n_hidden=100,\n",
    "    encoder=\"latency\",                 # сначала Poisson\n",
    "    top_k=0,                           # WTA выкл. для диагностики\n",
    "    enable_inhibition_at_start=False,  # ингибицию включим позже\n",
    ")\n",
    "res = run_experiment(cfg, verbose=True)\n",
    "print(\"\\nSUMMARY:\", {k: res[k] for k in [\"spikes_per_sample\",\"winners_unique\",\"winner_HHI\",\"energy_proxy_per_sample\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67314ca5-f60c-435e-945a-7594a098bfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
